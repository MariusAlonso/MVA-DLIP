{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights='ResNet34_Weights.IMAGENET1K_V1')  # New PyTorch interface for loading weights!\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224). \n",
    "\n",
    "First convolution layer with maxpool. \n",
    "Then 4 ResNet blocks. \n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7). \n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
    "\n",
    "##classes is a dictionary with the name of each class \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # Note: If the inverse normalisation is required, apply 1/x to the above object\n",
    "    \n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# if not os.path.exists(\"data\"):\n",
    "#     os.mkdir(\"data\")\n",
    "# if not os.path.exists(\"data/TP2_images\"):\n",
    "#     os.mkdir(\"data/TP2_images\")\n",
    "#     !cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 7\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
    " + More on [autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) and [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)\n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | - \n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_CAM(model, input_image):\n",
    "    # compute first three top classes\n",
    "    output = model(input_image)\n",
    "    target_values, target_indices = torch.topk(output, 3)\n",
    "    target_values = target_values[0].detach().numpy()\n",
    "    target_indices = target_indices[0].detach().numpy()\n",
    "    target_probabilities = (\n",
    "        F.softmax(torch.tensor(target_values), dim=0).detach().numpy()\n",
    "    )\n",
    "\n",
    "    print(\"Top 3-classes:\", target_indices, [classes[x] for x in target_indices])\n",
    "    print(\"Raw class scores:\", target_values)\n",
    "    print(\"Probabilities:\", target_probabilities)\n",
    "\n",
    "    # register hooks\n",
    "    activation = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output\n",
    "\n",
    "        return hook\n",
    "\n",
    "    activation_grad = {}\n",
    "\n",
    "    def get_activation_grad(name):\n",
    "        def hook(model, grad_input, grad_output):\n",
    "            activation_grad[name] = grad_output\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for module_name, module in dict(model.named_modules()).items():\n",
    "        module.register_forward_hook(get_activation(module_name))\n",
    "        module.register_backward_hook(get_activation_grad(module_name))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(target_indices) + 1, figsize=(15, 5))\n",
    "\n",
    "    # unnormalize input image\n",
    "    input_image_unnormalized = input_image.detach().numpy().squeeze().transpose(1, 2, 0)\n",
    "    input_image_unnormalized = input_image_unnormalized * [0.229, 0.224, 0.225]\n",
    "    input_image_unnormalized = input_image_unnormalized + [0.485, 0.456, 0.406]\n",
    "    input_image_unnormalized = input_image_unnormalized * 255.0\n",
    "\n",
    "    # show the orignal image\n",
    "    axes[0].imshow(input_image_unnormalized.astype(np.uint8))\n",
    "    axes[0].set_title(\"Original image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    for it, target_index in enumerate(target_indices):\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(input_image)\n",
    "\n",
    "        # backward pass\n",
    "        output_grad = torch.zeros_like(output)\n",
    "        output_grad[0, target_index] = 1\n",
    "        output.backward(gradient=output_grad)\n",
    "\n",
    "        # get the last convolutional layer\n",
    "        last_conv_layer = activation[\"layer4\"][0]\n",
    "        # get the gradients of the last convolutional layer\n",
    "        last_conv_layer_grad = activation_grad[\"layer4\"][0]\n",
    "        # compute the neuron importance weights\n",
    "        neuron_importance_weights = last_conv_layer_grad.mean(axis=(0, 2, 3))\n",
    "        # compute the weighted combination of the convolutional maps\n",
    "        weighted_combination = (\n",
    "            last_conv_layer * neuron_importance_weights.reshape(-1, 1, 1)\n",
    "        ).sum(axis=0)\n",
    "        # go through ReLU\n",
    "        weighted_combination = np.maximum(weighted_combination.detach().numpy(), 0)\n",
    "        # normalize the heatmap\n",
    "        weighted_combination = (weighted_combination - weighted_combination.min()) / (\n",
    "            weighted_combination.max() - weighted_combination.min()\n",
    "        )\n",
    "        # resize the heatmap to the size of the input image\n",
    "        weighted_combination = Image.fromarray(\n",
    "            (weighted_combination * 255).astype(np.uint8)\n",
    "        ).resize(input_image.shape[-2:])\n",
    "\n",
    "        # show the Grad-CAM output\n",
    "        axes[it + 1].imshow(weighted_combination, cmap=\"jet\", alpha=0.6)\n",
    "        axes[it + 1].imshow(input_image_unnormalized.astype(np.uint8), alpha=0.4)\n",
    "        axes[it + 1].set_title(\n",
    "            f\"Target index: {target_index} ({classes[target_index]}) | Probability: {target_probabilities[it]:.2f}\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "        axes[it + 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Grad_CAM\n",
    "index = 7\n",
    "heatmap = Grad_CAM(resnet34, dataset[index][0].view(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the picture of index 7, we clearly notice that the heatmaps of the first 3 labels are centered on the heads of the two dogs. We can infer that the head is the most important feature to identify the subject of the picture as a dog.\n",
    "\n",
    "The label which comes first, \"Great Dane\", has a heatmap that is more focused on the head of the dog in the foreground, while the third label, \"Chesapeak Bay Retriever\", has a heatmap that is more focused on the head of the dog in the background. Indeed, it seems that the dog in the foreground is a Great Dane, whereas the dog in the background is a Chesapeak Bay Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "### Complementary questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "##### Try GradCAM on others convolutional layers, describe and comment the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a Grad_CAM_bis() function that takes the layer that should be used for GradCAM as an input parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_CAM_bis(model, input_image, layer=\"layer4\", number_of_classes=3):\n",
    "    output = model(input_image)\n",
    "    target_values, target_indices = torch.topk(output, number_of_classes)\n",
    "    target_values = target_values[0].detach().numpy()\n",
    "    target_indices = target_indices[0].detach().numpy()\n",
    "    target_probabilities = (\n",
    "        F.softmax(torch.tensor(target_values), dim=0).detach().numpy()\n",
    "    )\n",
    "\n",
    "    print(\"Top 3-classes:\", target_indices, [classes[x] for x in target_indices])\n",
    "    print(\"Raw class scores:\", target_values)\n",
    "    print(\"Probabilities:\", target_probabilities)\n",
    "\n",
    "    activation = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output\n",
    "\n",
    "        return hook\n",
    "\n",
    "    activation_grad = {}\n",
    "\n",
    "    def get_activation_grad(name):\n",
    "        def hook(model, grad_input, grad_output):\n",
    "            activation_grad[name] = grad_output\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for module_name, module in dict(model.named_modules()).items():\n",
    "        module.register_forward_hook(get_activation(module_name))\n",
    "        module.register_backward_hook(get_activation_grad(module_name))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(target_indices) + 1, figsize=(15, 5))\n",
    "\n",
    "    input_image_unnormalized = input_image.detach().numpy().squeeze().transpose(1, 2, 0)\n",
    "    input_image_unnormalized = input_image_unnormalized * [0.229, 0.224, 0.225]\n",
    "    input_image_unnormalized = input_image_unnormalized + [0.485, 0.456, 0.406]\n",
    "    input_image_unnormalized = input_image_unnormalized * 255.0\n",
    "\n",
    "    axes[0].imshow(input_image_unnormalized.astype(np.uint8))\n",
    "    axes[0].set_title(\"Original image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    for it, target_index in enumerate(target_indices):\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(input_image)\n",
    "\n",
    "        output_grad = torch.zeros_like(output)\n",
    "        output_grad[0, target_index] = 1\n",
    "        output.backward(gradient=output_grad)\n",
    "\n",
    "        ### NEW\n",
    "        # get convolutional layer that is asked by user\n",
    "        asked_conv_layer = activation[layer][0]\n",
    "        ### NEW\n",
    "        # get the gradients of the convolutional layer that is asked by user\n",
    "        asked_conv_layer_grad = activation_grad[layer][0]\n",
    "\n",
    "        neuron_importance_weights = asked_conv_layer_grad.mean(axis=(0, 2, 3))\n",
    "        weighted_combination = (\n",
    "            asked_conv_layer * neuron_importance_weights.reshape(-1, 1, 1)\n",
    "        ).sum(axis=0)\n",
    "        weighted_combination = np.maximum(weighted_combination.detach().numpy(), 0)\n",
    "        weighted_combination = (weighted_combination - weighted_combination.min()) / (\n",
    "            weighted_combination.max() - weighted_combination.min()\n",
    "        )\n",
    "        weighted_combination = Image.fromarray(\n",
    "            (weighted_combination * 255).astype(np.uint8)\n",
    "        ).resize(input_image.shape[-2:])\n",
    "\n",
    "        axes[it + 1].imshow(weighted_combination, cmap=\"jet\", alpha=0.6)\n",
    "        axes[it + 1].imshow(input_image_unnormalized.astype(np.uint8), alpha=0.4)\n",
    "        axes[it + 1].set_title(\n",
    "            f\"Target index: {target_index} ({classes[target_index]}) | Probability: {target_probabilities[it]:.2f}\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "        axes[it + 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 7\n",
    "layer_list = [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
    "for layer in layer_list:\n",
    "    print(f\"Layer used for GradCAM: {layer}\")\n",
    "    Grad_CAM_bis(resnet34, dataset[index][0].view(1, 3, 224, 224), layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the layer 3, Grad-CAM focuses both on the dogs and on the humans in the background. It could be because the network learnt an association between dogs and humans from the training dataset. On the layer 4 however, there is no activation related to humans, which could lead to think that the deepest layer in the network managed to properly distinguish dogs from humans in the background. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at another example: in this horse picture, the 4th predicted class is \"plow, plough\".\n",
    "\n",
    "Examples of pictures from the \"plow\" class can be found on this link: https://salient-imagenet.cs.umd.edu/explore/class_730/feature_503.html\n",
    "\n",
    "Looking at other pictures from this class, we understand the link between \"horse\" and \"plow\" that comes from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "layer_list = [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
    "for layer in layer_list:\n",
    "    print(f\"Layer used for GradCAM: {layer}\")\n",
    "    Grad_CAM_bis(resnet34, dataset[index][0].view(1, 3, 224, 224), layer=layer, number_of_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the layer 1:\n",
    "\n",
    "Here, the class \"plow\" is triggered by the grass and the eye of the horse. It can be infered that the network learned that \"plow\" was linked to some grass (a field) and a large animal, like a cow or a horse, that has big black eyes.\n",
    "\n",
    "The class \"sorrel\" (horse) is triggered by some very very specific features of the horse, in a very localized manner.\n",
    "\n",
    "On layer 2 however, the class \"sorrel\" is triggered by a larger set of features, notably the contours of the horse, the legs, and the white dot on the horse's forehead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "15R9q3pPHMcH"
   },
   "source": [
    "##### What are the principal contributions of GradCAM (the answer is in the paper) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM uses the gradients of a label up until the final convolutional layer to produce a heatmap highlighting the important regions in the image for predicting the label.\n",
    "The principal contributions of GradCAM are : \n",
    "- Grad-CAM is applicable to a wide variety of CNN model-families without architectural changes or re-training: \n",
    "    - (1) CNNs with fullyconnected layers (e.g. VGG)\n",
    "    - (2) CNNs used for structured outputs (e.g. captioning)\n",
    "    - (3) CNNs used in tasks with multimodal inputs (e.g. visual question answering) or reinforcement learning\n",
    "- GradCAM visualizations help in diagnosing failure modes by uncovering biases in datasets\n",
    "- Interpretability: Grad-CAM visualizations outperform existing approaches in terms of interpretability. They can accurately discriminate between classes, are fairly robust to adversarial noise, expose the trustworthiness of a classifier, and help identify biases in datasets\n",
    "- Broad Applicability: Grad-CAM is applicable to various off-the-shelf architectures for tasks such as image classification, image captioning, and visual question answering, showcasing its versatility and utility across different domains\n",
    "- Combining High-Resolution and Class-Discriminative Visualizations: By combining Grad-CAM with existing pixel-space gradient visualizations, such as Guided Backpropagation and Deconvolution, Grad-CAM creates Guided Grad-CAM visualizations that are both high-resolution and class-discriminative, providing detailed and accurate visual explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "15R9q3pPHMcH",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
